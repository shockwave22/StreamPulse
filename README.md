# ğŸ“º StreamPulse: Netflix Sentiment Dashboard

A comprehensive data pipeline and dashboard application that ingests social media data and survey data, performs sentiment analysis, and displays consumer insights about Netflix original content over time.

## ğŸš€ Features

- **Multi-Platform Data Ingestion**: Twitter, Reddit, and Survey data collection
- **Advanced Sentiment Analysis**: VADER and Transformer-based models
- **Real-time Dashboard**: Interactive Streamlit interface
- **Automated Pipeline**: Airflow orchestration for scheduled data processing
- **Comprehensive Analytics**: Trend analysis, comparison views, and keyword insights

## ğŸ“‹ Requirements

- Python 3.9+
- PostgreSQL (optional, SQLite included for development)
- Twitter Developer Account
- Reddit API Access

## ğŸ› ï¸ Quick Start

### 1. Clone and Setup

```bash
git clone https://github.com/shockwave22/StreamPulse.git
cd StreamPulse

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Configure Environment

Create a `.env` file with your API credentials:

```bash
# Database
DATABASE_URL=sqlite:///streampulse.db

# Twitter API
TWITTER_BEARER_TOKEN=your_bearer_token
TWITTER_API_KEY=your_api_key
TWITTER_API_SECRET=your_api_secret
TWITTER_ACCESS_TOKEN=your_access_token
TWITTER_ACCESS_TOKEN_SECRET=your_access_token_secret

# Reddit API
REDDIT_CLIENT_ID=your_client_id
REDDIT_CLIENT_SECRET=your_client_secret
```

### 3. Initialize and Run

```bash
# Setup database and collect initial data
python main.py --full

# Start the dashboard
streamlit run dashboard/app.py
```

The dashboard will be available at `http://localhost:8501`

## ğŸ³ Docker Deployment

```bash
# Start all services
docker-compose up -d

# Access dashboard at http://localhost:8501
# Access Airflow at http://localhost:8080
```

## ğŸ“Š Dashboard Features

### Overview Tab
- Total posts/comments metrics
- Average sentiment scores
- Survey satisfaction ratings
- Sentiment distribution charts

### Trends Tab
- Sentiment trends over time
- Platform-specific breakdowns
- Interactive time series visualizations

### Comparison Tab
- Survey vs social media sentiment
- Alignment analysis
- Key insights and recommendations

### Keywords Tab
- Word clouds from recent content
- Top trending keywords
- Theme analysis

### Raw Data Tab
- Recent tweets and comments
- Engagement metrics
- Content filtering options

## ğŸ”„ Data Pipeline

The pipeline consists of four main stages:

1. **Data Ingestion**: Collect data from Twitter, Reddit, and survey sources
2. **Sentiment Analysis**: Process text using VADER or transformer models
3. **Data Aggregation**: Calculate daily metrics and trends
4. **Dashboard Update**: Refresh visualizations and insights

### Scheduled Pipeline (Airflow)

The pipeline runs automatically every 6 hours:
- Fetch new social media content
- Process sentiment analysis
- Update aggregated metrics
- Refresh dashboard data

## ğŸ“ Project Structure

```
StreamPulse/
â”œâ”€â”€ config.py                 # Configuration settings
â”œâ”€â”€ main.py                   # Main application entry point
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ Dockerfile               # Docker configuration
â”œâ”€â”€ docker-compose.yml       # Multi-service deployment
â”œâ”€â”€ database/
â”‚   â”œâ”€â”€ models.py            # SQLAlchemy models
â”‚   â””â”€â”€ database.py          # Database connection
â”œâ”€â”€ data_ingestion/
â”‚   â”œâ”€â”€ twitter_collector.py # Twitter API integration
â”‚   â”œâ”€â”€ reddit_collector.py  # Reddit API integration
â”‚   â””â”€â”€ survey_collector.py  # Survey data handling
â”œâ”€â”€ sentiment_analysis/
â”‚   â””â”€â”€ analyzer.py          # Sentiment analysis engine
â”œâ”€â”€ data_aggregation/
â”‚   â””â”€â”€ aggregator.py        # Data aggregation logic
â”œâ”€â”€ dashboard/
â”‚   â””â”€â”€ app.py               # Streamlit dashboard
â”œâ”€â”€ airflow_dags/
â”‚   â””â”€â”€ streampulse_pipeline.py # Airflow DAG
â”œâ”€â”€ data/                    # Data storage directory
â””â”€â”€ logs/                    # Application logs
```

## ğŸ”§ Configuration

### Netflix Titles Tracked
- Stranger Things
- The Witcher
- Wednesday
- Ozark
- The Crown
- Bridgerton
- Squid Game
- Money Heist
- Dark
- The Umbrella Academy

### Sentiment Analysis Models
- **VADER**: Rule-based sentiment analysis (default)
- **Transformers**: DistilBERT fine-tuned model (optional)

## ğŸ“ˆ Usage Examples

### Manual Data Collection
```bash
# Collect data for specific operations
python main.py --collect    # Collect new data
python main.py --sentiment  # Process sentiment
python main.py --aggregate  # Update aggregations
```

### API Integration
```python
from data_ingestion.twitter_collector import TwitterCollector

collector = TwitterCollector()
tweets = collector.collect_tweets_for_title("Stranger Things", max_results=50)
```

## ğŸ§ª Testing

```bash
# Run tests
pytest

# Run specific test modules
pytest tests/test_sentiment_analysis.py
pytest tests/test_data_ingestion.py
```

## ğŸ”’ Security Considerations

- Store API credentials in environment variables
- Use read-only database credentials for dashboard
- Implement rate limiting for API calls
- Regular security updates for dependencies

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ†˜ Troubleshooting

### Common Issues

**Database Connection Errors**
- Verify DATABASE_URL in .env file
- Ensure PostgreSQL is running (if using)
- Check database permissions

**API Rate Limits**
- Monitor API usage in logs
- Implement longer delays between requests
- Consider upgrading API plans

**Dashboard Not Loading**
- Check if all dependencies are installed
- Verify Streamlit is running on correct port
- Review application logs for errors

### Getting Help

- Check the [Issues](https://github.com/shockwave22/StreamPulse/issues) page
- Review logs in the `logs/` directory
- Enable debug logging: `LOG_LEVEL=DEBUG`

## ğŸ“Š Performance Tips

- Use PostgreSQL for production deployments
- Configure database connection pooling
- Implement caching for dashboard data
- Monitor memory usage with large datasets
- Use batch processing for sentiment analysis

---

**Built with â¤ï¸ by the StreamPulse Team**